{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf872352",
   "metadata": {},
   "source": [
    "Mix ai generated gemini 2.5 pro and thibault lootvoet + copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ff626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Loading ---\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads all parquet files and performs initial type conversion.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        # Load the single-file datasets\n",
    "        flights_df = pd.read_parquet(\"data/flightlist_train.parquet\")\n",
    "        fuel_df = pd.read_parquet(\"data/fuel_train.parquet\")\n",
    "        airports_df = pd.read_parquet(\"data/apt.parquet\")\n",
    "\n",
    "        print(\"Loaded flight list, fuel, and airport data.\")\n",
    "\n",
    "        # --- Load all trajectory files from the 'flights_train' directory ---\n",
    "\n",
    "        traj_path = \"data/flights_train\"\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not os.path.isdir(traj_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Trajectory directory not found: '{traj_path}'. \"\n",
    "                \"Please ensure it's in the same location as the script.\"\n",
    "            )\n",
    "\n",
    "        # Use glob to find all .parquet files in the directory\n",
    "        all_traj_files = glob.glob(os.path.join(traj_path, \"*.parquet\"))\n",
    "\n",
    "        # keep only the specified files for testing\n",
    "        # all_traj_files = all_traj_files[start:end]\n",
    "\n",
    "        if not all_traj_files:\n",
    "            raise FileNotFoundError(\n",
    "                f\"No .parquet files found in '{traj_path}' directory.\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"Found {len(all_traj_files)} trajectory files. Loading and concatenating...\"\n",
    "        )\n",
    "\n",
    "        # Create a list of dataframes, one for each file\n",
    "        traj_dfs = [pd.read_parquet(f) for f in all_traj_files]\n",
    "\n",
    "        # Concatenate all of them into a single dataframe\n",
    "        traj_df = pd.concat(traj_dfs, ignore_index=True)\n",
    "\n",
    "        print(\"Trajectory data concatenated successfully.\")\n",
    "\n",
    "        # --- 2. Data Preprocessing and Cleaning (same as before) ---\n",
    "        print(\"Preprocessing data...\")\n",
    "\n",
    "        # Convert all relevant timestamps to datetime objects\n",
    "        flights_df[\"takeoff\"] = pd.to_datetime(flights_df[\"takeoff\"])\n",
    "        flights_df[\"landed\"] = pd.to_datetime(flights_df[\"landed\"])\n",
    "\n",
    "        traj_df[\"timestamp\"] = pd.to_datetime(traj_df[\"timestamp\"])\n",
    "\n",
    "        fuel_df[\"start\"] = pd.to_datetime(fuel_df[\"start\"])\n",
    "        fuel_df[\"end\"] = pd.to_datetime(fuel_df[\"end\"])\n",
    "\n",
    "        # \"Quick and Dirty\" cleaning: Drop flights with missing aircraft_type\n",
    "        flights_df = flights_df.dropna(subset=[\"aircraft_type\"])\n",
    "\n",
    "        return flights_df, traj_df, fuel_df, airports_df\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\n",
    "            \"Please make sure all files (apt.parquet, flightlist_train.parquet, fuel_train.parquet) \"\n",
    "            \"and the directory (flights_train/) are in the same location.\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data loading: {e}\")\n",
    "        print(\"You may need to install a parquet engine, e.g.: pip install pyarrow\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Feature Engineering ---\n",
    "\n",
    "\n",
    "def create_features(\n",
    "    flights_df: pd.DataFrame,\n",
    "    traj_df: pd.DataFrame,\n",
    "    fuel_df: pd.DataFrame,\n",
    "    airports_df: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame, pd.Series, list]:\n",
    "    \"\"\"\n",
    "    Engineers a model-ready feature set (X) and target (y) from the raw data.\n",
    "\n",
    "    The core logic is built around the `fuel_df`, where each row represents\n",
    "    a single fuel-burn interval and becomes one sample for the model.\n",
    "\n",
    "    This function performs three main steps:\n",
    "    1.  Merges Static Data: Enriches each fuel interval with static\n",
    "        data from `flights_df` (e.g., aircraft_type) and `airports_df`\n",
    "        (e.g., origin/destination elevation).\n",
    "    2.  Filters & Aggregates Time-Series: For each interval, it finds all\n",
    "        trajectory points from `traj_df` that fall *within* its [start, end]\n",
    "        timestamp. It then aggregates these points to create summary\n",
    "        statistics (e.g., mean_altitude, max_TAS, n_points).\n",
    "    3.  Cleans & Finalizes: Calculates delta features (e.g., altitude_change),\n",
    "        handles missing values, and formats the final feature matrix `X`\n",
    "        and target vector `y`.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): DataFrame of features for the model.\n",
    "        y (pd.Series): Series of the target variable (fuel_kg).\n",
    "        categorical_features (list): List of column names to be\n",
    "                                     treated as categorical.\n",
    "    \"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "\n",
    "    # 3.1: Create base table from fuel data\n",
    "    # Each row in fuel_df is our target sample\n",
    "    data = fuel_df.copy()\n",
    "\n",
    "    # 3.2: Add flight-level features\n",
    "    data = pd.merge(\n",
    "        data,\n",
    "        flights_df[[\"flight_id\", \"aircraft_type\", \"origin_icao\", \"destination_icao\"]],\n",
    "        on=\"flight_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # 3.3: Add airport features (elevation)\n",
    "    airports_df_simple = airports_df[[\"icao\", \"elevation\"]].rename(\n",
    "        columns={\"elevation\": \"origin_elevation\"}\n",
    "    )\n",
    "    data = pd.merge(\n",
    "        data, airports_df_simple, left_on=\"origin_icao\", right_on=\"icao\", how=\"left\"\n",
    "    )\n",
    "    airports_df_simple = airports_df_simple.rename(\n",
    "        columns={\"origin_elevation\": \"dest_elevation\"}\n",
    "    )\n",
    "    data = pd.merge(\n",
    "        data,\n",
    "        airports_df_simple,\n",
    "        left_on=\"destination_icao\",\n",
    "        right_on=\"icao\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # 3.4: Engineer interval-based trajectory features (The complex part)\n",
    "\n",
    "    # Give each interval a unique ID to group by\n",
    "    data[\"interval_id\"] = data.index\n",
    "\n",
    "    # Merge all trajectory points with all intervals for the same flight\n",
    "    # This creates a large table, but allows vectorized filtering.\n",
    "    merged_traj = pd.merge(\n",
    "        traj_df, data[[\"flight_id\", \"start\", \"end\", \"interval_id\"]], on=\"flight_id\"\n",
    "    )\n",
    "\n",
    "    # Filter: Keep only trajectory points that fall *within* the [start, end] of a fuel interval\n",
    "    filtered_traj = merged_traj[\n",
    "        (merged_traj[\"timestamp\"] >= merged_traj[\"start\"])\n",
    "        & (merged_traj[\"timestamp\"] <= merged_traj[\"end\"])\n",
    "    ].copy()\n",
    "\n",
    "    # 3.5: Aggregate trajectory features by interval\n",
    "    print(\"Aggregating trajectory features...\")\n",
    "\n",
    "    # Sort by time to get first/last points correctly\n",
    "    filtered_traj.sort_values(by=[\"interval_id\", \"timestamp\"], inplace=True)\n",
    "\n",
    "    # Simple aggregates\n",
    "    agg_features = filtered_traj.groupby(\"interval_id\").agg(\n",
    "        mean_altitude=(\"altitude\", \"mean\"),\n",
    "        max_altitude=(\"altitude\", \"max\"),\n",
    "        mean_TAS=(\"TAS\", \"mean\"),\n",
    "        max_TAS=(\"TAS\", \"max\"),\n",
    "        mean_groundspeed=(\"groundspeed\", \"mean\"),\n",
    "        mean_vertical_rate=(\"vertical_rate\", \"mean\"),\n",
    "        n_points=(\"timestamp\", \"count\"),\n",
    "    )\n",
    "\n",
    "    # Get features from the *first* and *last* point in the interval\n",
    "    first_points = filtered_traj.groupby(\"interval_id\").first()\n",
    "    last_points = filtered_traj.groupby(\"interval_id\").last()\n",
    "\n",
    "    delta_features = pd.DataFrame(index=agg_features.index)\n",
    "    delta_features[\"altitude_change\"] = (\n",
    "        last_points[\"altitude\"] - first_points[\"altitude\"]\n",
    "    )\n",
    "    delta_features[\"time_in_interval_s\"] = (\n",
    "        last_points[\"timestamp\"] - first_points[\"timestamp\"]\n",
    "    ).dt.total_seconds()\n",
    "\n",
    "    # 3.6: Merge aggregated features back into the main table\n",
    "    data = data.merge(agg_features, on=\"interval_id\", how=\"left\")\n",
    "    data = data.merge(delta_features, on=\"interval_id\", how=\"left\")\n",
    "\n",
    "    # 3.7: Add simple features\n",
    "    # This is the 'reported' duration, which is a key predictor\n",
    "    data[\"interval_duration_s\"] = (data[\"end\"] - data[\"start\"]).dt.total_seconds()\n",
    "\n",
    "    # --- 4. Final Processing ---\n",
    "    print(\"Finalizing dataset...\")\n",
    "\n",
    "    # Define feature set\n",
    "    # We must convert aircraft_type to a category for LightGBM\n",
    "    data[\"aircraft_type\"] = data[\"aircraft_type\"].astype(\"category\")\n",
    "\n",
    "    # \"Quick and Dirty\" cleaning:\n",
    "    # Drop rows where we have no fuel target\n",
    "    data = data.dropna(subset=[\"fuel_kg\"])\n",
    "    # Drop rows where interval duration is zero or negative\n",
    "    data = data[data[\"interval_duration_s\"] > 0]\n",
    "    # Drop intervals that had no trajectory points (n_points is NaN after left merge)\n",
    "    data = data.dropna(subset=[\"n_points\"])\n",
    "\n",
    "    # Fill any other remaining NaNs (e.g., missing TAS) with 0\n",
    "    # A better approach would be interpolation or 0-filling only specific columns\n",
    "    # Step 1: Handle the categorical column ('aircraft_type').\n",
    "    # Convert it to a standard 'object' (string) column first. This \"unlocks\" it.\n",
    "    data[\"aircraft_type\"] = data[\"aircraft_type\"].astype(\"object\")\n",
    "\n",
    "    # Step 2: Now that it's a simple object column, fill the NaNs.\n",
    "    # We use 'UNKNOWN' as the placeholder.\n",
    "    data[\"aircraft_type\"] = data[\"aircraft_type\"].fillna(\"UNKNOWN\")\n",
    "\n",
    "    # Step 3: Convert the fully-filled column back to 'category'.\n",
    "    # Pandas will now see 'UNKNOWN' as one of the valid categories.\n",
    "    data[\"aircraft_type\"] = data[\"aircraft_type\"].astype(\"category\")\n",
    "\n",
    "    # Step 4: Now, fill all remaining *numeric* columns with 0.\n",
    "    numeric_cols = data.select_dtypes(include=np.number).columns\n",
    "    data[numeric_cols] = data[numeric_cols].fillna(0)\n",
    "\n",
    "    features = [\n",
    "        \"interval_duration_s\",\n",
    "        \"aircraft_type\",\n",
    "        \"origin_elevation\",\n",
    "        \"dest_elevation\",\n",
    "        \"mean_altitude\",\n",
    "        \"max_altitude\",\n",
    "        \"mean_TAS\",\n",
    "        \"max_TAS\",\n",
    "        \"mean_groundspeed\",\n",
    "        \"mean_vertical_rate\",\n",
    "        \"n_points\",\n",
    "        \"altitude_change\",\n",
    "        \"time_in_interval_s\",\n",
    "    ]\n",
    "    categorical_features = [\"aircraft_type\"]\n",
    "    target = \"fuel_kg\"\n",
    "\n",
    "    # Filter out any rows that somehow still have NaN/Inf in features\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "\n",
    "    # Define our numeric features (which is all features *except* the categorical ones)\n",
    "    numeric_features = [f for f in features if f not in categorical_features]\n",
    "\n",
    "    # Apply numeric-only cleaning (replace inf, fillna)\n",
    "    # ONLY to the numeric columns.\n",
    "    X[numeric_features] = (\n",
    "        X[numeric_features].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset finalized. Total samples: {len(X)}\")\n",
    "    print(f\"Features: {features}\")\n",
    "\n",
    "    display(X.head())\n",
    "    display(y.head())\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "    return X, y, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0beb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Model Training with Optuna ---\n",
    "\n",
    "\n",
    "def train_model(X, y, categorical_features, n_trials=50, to_load=None):\n",
    "    \"\"\"Sets up Optuna to find the best LightGBM hyperparameters.\"\"\"\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"Optuna objective function to minimize.\"\"\"\n",
    "\n",
    "        # Hyperparameters to tune\n",
    "        params = {\n",
    "            \"objective\": \"regression_l1\",  # MAE is less sensitive to outliers than MSE (L2)\n",
    "            \"metric\": \"rmse\",\n",
    "            \"n_estimators\": 1000,\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 20),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
    "            \"verbose\": -1,\n",
    "            \"n_jobs\": -1,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "            categorical_feature=categorical_features,\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        return rmse\n",
    "\n",
    "    print(\"\\n--- Starting Optuna Hyperparameter Search ---\")\n",
    "    if to_load:\n",
    "        study = optuna.load_study(study_name=to_load, storage=\"sqlite:///db.sqlite3\")\n",
    "        print(f\"Loaded existing study '{to_load}' with {len(study.trials)} trials.\")\n",
    "    else:\n",
    "        study = optuna.create_study(direction=\"minimize\", study_name=f\"LightGBM Fuel Burn Prediction {datetime.datetime.now().isoformat()}\", storage=\"sqlite:///db.sqlite3\", load_if_exists=True)\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\nOptuna search finished.\")\n",
    "    print(f\"Best RMSE on validation set: {study.best_value:.4f}\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    # --- 6. Final Model Evaluation ---\n",
    "    print(\"\\n--- Training Final Model on All Training Data ---\")\n",
    "\n",
    "    # Add n_estimators from the best trial (or use the fitted one from early stopping)\n",
    "    best_params = study.best_params\n",
    "    best_params[\"n_estimators\"] = 1000  # We will use early stopping\n",
    "\n",
    "    final_model = lgb.LGBMRegressor(\n",
    "        objective=\"regression_l1\",\n",
    "        metric=\"rmse\",\n",
    "        seed=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "        **best_params,\n",
    "    )\n",
    "\n",
    "    final_model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"rmse\",\n",
    "        callbacks=[lgb.early_stopping(100, verbose=True)],\n",
    "        categorical_feature=categorical_features,\n",
    "    )\n",
    "\n",
    "    val_preds = final_model.predict(X_val)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    print(f\"\\nFinal Model Validation RMSE: {final_rmse:.4f}\")\n",
    "\n",
    "    # Show feature importances\n",
    "    try:\n",
    "        lgb.plot_importance(final_model, max_num_features=20)\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.savefig(\"feature_importance.png\", bbox_inches=\"tight\")\n",
    "        print(\"\\nSaved feature importance plot to 'feature_importance.png'\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"\\nCould not plot feature importance. (Is matplotlib installed?) Error: {e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94febebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X, y, categorical_features, model_to_load):\n",
    "    \"\"\"Loads the best model from the Optuna study.\"\"\"\n",
    "    study = optuna.load_study(study_name=model_to_load, storage=\"sqlite:///db.sqlite3\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best trial: {best_trial.number}\")\n",
    "    print(f\"Best value (RMSE): {best_trial.value:.4f}\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Train final model with best hyperparameters\n",
    "    final_model = lgb.LGBMRegressor(best_trial.params)\n",
    "    final_model.fit(X, y, categorical_feature=categorical_features)\n",
    "\n",
    "    # print rmse on the training set\n",
    "    train_preds = final_model.predict(X)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y, train_preds))\n",
    "    print(f\"\\nFinal Model Training RMSE: {train_rmse:.4f}\")\n",
    "    \n",
    "    # Show feature importances\n",
    "    try:\n",
    "        lgb.plot_importance(final_model, max_num_features=20)\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.savefig(\"feature_importance.png\", bbox_inches=\"tight\")\n",
    "        print(\"\\nSaved feature importance plot to 'feature_importance.png'\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"\\nCould not plot feature importance. (Is matplotlib installed?) Error: {e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nécessité de faire du feature engineering i.e. prendre des moyennes ou qqch parce que là ce n'est pas possible\n",
    "# approche actuelle: https://gemini.google.com/app/2db88ca378891ac5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights, traj, fuel, airports = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the first 100 rows of fuel for faster testing\n",
    "# fuel_short = fuel.iloc[:200].reset_index(drop=True)\n",
    "# fuel_short = fuel.copy()\n",
    "\n",
    "X_features, y_target, categorical_cols = create_features(flights, traj, fuel, airports)\n",
    "train_model(X_features, y_target, categorical_cols, n_trials=100)\n",
    "# load_model(X_features, y_target, categorical_cols, model_to_load=\"LightGBM Fuel Burn Prediction 2025-10-28T21:03:59.935861\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(X_features, y_target, categorical_cols, model_to_load=\"LightGBM Fuel Burn Prediction 2025-10-30T12:42:38.153171\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(X_features, y_target, categorical_cols, n_trials=50, to_load=\"LightGBM Fuel Burn Prediction 2025-10-30T12:42:38.153171\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
